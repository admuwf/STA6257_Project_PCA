---
title: "Principal Component Analysis"
format: revealjs
editor: visual
author: Andrew Mercer, Curtis Fox, Eddie Pierce
---

## Intoduction
* Principal Component Analysis (PCA) is a widely used statistical method for reducing the dimensionality of multivariate data.

* The goal is to transform the original variables into a new set of uncorrelated variables called principal components.

* The Principal Components are used in place of the original variables to reduce data dimensionality to find more accurate results and identify the most important factors that contribute to overall performance.

## How It Works in Simple Terms
* The data is plotted along a normal x and y graph. The average distance to each axis is used to find the center of the data. The center is then shifted to the origin.

* PC1 is the line that is fitted to the shifted data through the origin using the maximum sum of squared distances it calculated. PC2 is then found by creating a scaled perpendicular line to the fitted line through the origin. The lines are rotated to the axis.

* The linear combination of the slope of each line is used to find the eigenvalues and eigenvectors. The proportion of each variable used to create the line is the loading score.


## What You Need for PCA

* Your data needs to be Numeric.

* Multiple variables, 4 or more.

* A Pre-Determined Principal Component Cutoff Point

## Literature Review
* PCA has been applied in various fields such as remote sensing, public health, finance, sports, and climate analysis.

* Watnik, Mitchell, and Richard A Levine. 2001. “NFL Y2K PCA.” Journal of Statistics Education 9 (3).

* Jones, Lewis. 2016. “Modeling NFL Quarterback Success with College Data.” PhD thesis, University of Georgia.

* We found a shared interest in individual Quarterback performance across the National Football League. The data was numeric with multiple variables and we had a strong understanding of the data. 

## Methods

## Methods

## Data

2022 National Football League Quarterback Passing Data https://www.pro-football-reference.com/years/2022/passing.htm

```{r}
our_data = read.csv("NFL_Data.csv")

knitr::kable(
  our_data[1:3, 1:10], caption = 'Original 2022 NFL Data.'
)
```

## Data Continued




## Correlation Matrix
```{r}
our_data = read.csv("NFL_Data.csv")
library('ggplot2')
library('corrr')
library('ggcorrplot')
library('FactoMineR')
library('factoextra')
our_data[is.na(our_data)] = 0
numerical_data = our_data[,2:51]
data_normalized = scale(numerical_data)
corr_matrix = cor(data_normalized)
ggcorrplot(corr_matrix, tl.cex = 5,tl.srt = 45)

```

This table displays the correlation coefficients between the variables in a data set. The data in this correlation matrix can now be passed through the PCA function and graphed to show the Principal Components.

## Scree Plot
*
```{r}
data.pca = princomp(corr_matrix)
fviz_eig(data.pca, addlabels = TRUE) #this is a scree plot of the principal component

```

The Scree plot shows the eigenvalues each as a bar graph in a downward curve, from highest to lowest. As originally hoped, the first two components, can be considered the most significant since they contain almost 91% of the total information of the data. 


## Biplot
*
```{r}
fviz_pca_biplot(data.pca,
geom.ind="point",
col.ind="black",
pointshape=16,pointsize=2,
mean.point=FALSE,
alpha.var="contrib",col.var="cos2",
gradient.cols=c("#00AFBB","#E7B800","#FC4E07"),
labelsize=3,
repel=TRUE,
xlab="PC 1",
ylab="PC 2")
```



## Biplot 2
*
```{r}
fviz_pca_var(data.pca, col.var = "black")

```

The biplot can visualize the similarities and dissimilarities between the samples, and further shows the impact of each attribute on each principal component. The biplot observes three key pieces of information: all the variables that are grouped together are positively correlated to each other, the higher the distance between the variable and the origin, the better represented that variable is, and variables that are negatively correlated are displayed to the opposite sides of the biplot's origin.

## Cos2 Graph
*
```{r}
fviz_cos2(data.pca, choice = "var",axes = 1:2)

```

This visualization is to determine how much each variable is represented in a given component. A high value means that the variable is highly represented by that component.

## Combination BiPlot
*
```{r}
fviz_pca_var(data.pca, col.var = "cos2",
            gradient.cols = c("goldenrod1", "indianred",  "cornflowerblue"),
            repel = TRUE)

```

This graph uses the biplot graph and then enhances it with color based on the Cos2 values. The figure has 3 colors, but you can see them start to blend in areas where the Cos2 values are similar between the groups we talked about before.

## Conclusion
In conclusion, using the 2022 season data for the National Football League we were able to reduce dimensionality to 2 Principal Components. We used a Scree Plot to observe that our first 2 Principal Components explained 90.9% of the variance. This was 0.9% greater than our pre-determined cutoff.
