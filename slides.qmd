---
title: "Principal Component Analysis"
format: revealjs
editor: visual
author: Andrew Mercer, Curtis Fox, Eddie Pierce
---

## Intoduction
* Principal Component Analysis (PCA) is a widely used statistical method for reducing the dimensionality of multivariate data.

* The goal is to transform the original variables into a new set of uncorrelated variables called principal components.

* The Principal Components are used in place of the original variables to reduce data dimensionality to find more accurate results and identify the most important factors that contribute to overall performance.

## How It Works in Simple Terms
* The data is plotted along a normal x and y graph. The average distance to each axis is used to find the center of the data. The center is then shifted to the origin.

* PC1 is the line that is fitted to the shifted data through the origin using the maximum sum of squared distances it calculated. PC2 is then found by creating a scaled perpendicular line to the fitted line through the origin. The lines are rotated to the axis.

* The linear combination of the slope of each line is used to find the eigenvalues and eigenvectors. The proportion of each variable used to create the line is the loading score.


## What You Need for PCA

* Your data needs to be Numeric.

* Multiple variables, 4 or more.

* A Pre-Determined Principal Component Cutoff Point

## Literature Review
* PCA has been applied in various fields such as remote sensing, public health, finance, sports, and climate analysis.

* Watnik, Mitchell, and Richard A Levine. 2001. “NFL Y2K PCA.” Journal of Statistics Education 9 (3).

* Jones, Lewis. 2016. “Modeling NFL Quarterback Success with College Data.” PhD thesis, University of Georgia.

* We found a shared interest in individual Quarterback performance across the National Football League. The data was numeric with multiple variables and we had a strong understanding of the data. 

## Methods 
* Principal Component Analysis (PCA) is an unsupervised machine learning technique for reducing the dimensionality of multivariate data.
* A PCA analysis allows us to reduce the number of variables in a dataset
* First step is to normalize the data using equation 1 

$$ x_{new} = \frac{x - {\mu}}{\sigma}  \tag{1}$$

## Methods Continued
* Second step is to calculate the covariance matrix in the data set using equation 2
* We then need to find the eigenvalues and eigenvectors using equations 3 and 4
* Plot the average measurement of each variable to the origin

$$Cov(x,y) = \frac{1}{n-1}\sum_{i=1}^{n}(X_{i}-\bar{X})(Y_{i}-\bar{Y})^T\tag{2}$$


$$det(A - \lambda I) = 0 \tag{3}$$
$$(A - \lambda I) \cdot v = 0 \tag{4}$$
## Methods Continued
* The PCA function fits a line to the data using the maximum sum of squared distances to determine the first Principal Component
* PC2 is found using a perpendicular line to PC1 where it meets the origin.
* This process is continued until all variables have been completed or until a pre-determined number of components have been reached



## Data
* Explored NFL passing statistics from the 2022-2023 calendar year
* Analyzed over 50 variables in the data set 
* All variable types besides QB name were numeric

```{r}
our_data = read.csv("NFL_Data.csv")

knitr::kable(
  our_data[1:3, 1:10], caption = 'Original 2022 NFL Data.'
)
```

## Correlation Matrix
```{r}
our_data = read.csv("NFL_Data.csv")
library('ggplot2')
library('corrr')
library('ggcorrplot')
library('FactoMineR')
library('factoextra')
our_data[is.na(our_data)] = 0
numerical_data = our_data[,2:51]
data_normalized = scale(numerical_data)
corr_matrix = cor(data_normalized)
ggcorrplot(corr_matrix, tl.cex = 5,tl.srt = 45)

```

* Matrix of the correlations between of the variables in the data. Deep red is positively correlated, and deep blue is negatively correlated. 

## Scree Plot
```{r}
data.pca = princomp(corr_matrix)
fviz_eig(data.pca, addlabels = TRUE) #this is a scree plot of the principal component

```

* This shows the variation explained by each principal component in descending order.

## Biplot
```{r}
fviz_pca_biplot(data.pca,
geom.ind="point",
col.ind="black",
pointshape=16,pointsize=2,
mean.point=FALSE,
alpha.var="contrib",col.var="cos2",
gradient.cols=c("#00AFBB","#E7B800","#FC4E07"),
labelsize=3,
repel=TRUE,
xlab="PC 1",
ylab="PC 2")
```

* This biplot shows how each principal component is composed of each attribute from the data. 
* The points are each individual player, and the colors are the Cos2 values.

## Biplot 2
```{r}
fviz_pca_var(data.pca, col.var = "black")

```

* This biplot is a closer look of the impact of each attribute on the principal components. 

## Cos2 Graph
```{r}
fviz_cos2(data.pca, choice = "var",axes = 1:2)

```

* This graph shows the Cos2 values of each attribute.

## Combination BiPlot
```{r}
fviz_pca_var(data.pca, col.var = "cos2",
            gradient.cols = c("goldenrod1", "indianred",  "cornflowerblue"),
            repel = TRUE)

```
* This graph is to highlight more on the Cos2 values from before, as a comparison.

## Conclusion
In conclusion, using the 2022 season data for the National Football League we were able to reduce dimensionality to 2 Principal Components. We used a Scree Plot to observe that our first 2 Principal Components explained 90.9% of the variance. This was 0.9% greater than our pre-determined cutoff.
