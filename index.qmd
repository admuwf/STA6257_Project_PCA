---
title: "PCA Method Review"
author: "Andrew Mercer, Curtis Fox, Eddie Pierce"
date: '`r Sys.Date()`'
toc: True
format:
  html:
    code-fold: true
course: STA 6257 - Advance Statistical Modeling
bibliography: references.bib # file contains bibtex for references
#always_allow_html: true # this allows to get PDF with HTML features
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
```


```{r}
library(ggplot2)
library(corrr)
library(ggcorrplot)
library(FactoMineR)
library(factoextra)
```

### Introduction
Principal Component Analysis (PCA) is a widely used statistical method for reducing the dimensionality of multivariate data. The goal of PCA is to transform the original variables into a new set of uncorrelated variables called principal components. This literature review aims to explore the methods and applications of PCA used in previous data analysis. While we will review the use of PCA in several analytical fields, our use of PCA focuses on National Football League data. Our analysis can be found below.
In the simplest terms, PCA acts to rotate the axis of different variables to better understand the variance in the data. The variables with the most important relationships are kept as they explain the most variance in the dataset. In this way PCA is used to decrease the dimensionality of a dataset. The first Principal component is calculated by adding the variables once they have been multiplied by a weighted value. The next Principal component is calculated using another variable as long as that variable is uncorrelated to the first. This continues until the number of Principal Components equals the number of variables or until you have reached a determined cutoff point [@holland2008principal]. The four options provided by Holland to be used as a cutoff point are if the increase of variance explained from one variable to the next is minimal. The second is to set a limit for variance explained, possibly 90%, and ignore all PCs after this limit is reached. The third is to use a correlation matrix and the variance is less than 1. Finally, the fourth, is to ignore the last PCs in the list if their variance explained is equal. In all cases the distribution of the variables data needs to be normally distributed for PCA to work correctly.

PCA is a powerful tool that can be applied to seemingly any field. Our review has found the application of PCA in remote sensing, public health, finance, and sports. In a paper on remote sensing by [@kwarteng1989extracting], the author proposed a method for extracting spectral contrast from Landsat Thematic Mapper Image data. Chavez used Selective Principal Component Analysis (SPCA), a modification of the traditional PCA. This modification was used to extract principal components that are more informative for specific image analysis applications. The author demonstrated that SPCA can improve the discrimination between land cover classes in remote sensing applications. In the field of public health [@ngo2019principal] performed a systematic review and meta-analysis of morbidity and mortality among the United States homeless population using PCA. The authors applied PCA to a set of variables related to health outcomes among homeless individuals. The study showed that PCA can be a useful tool for summarizing complex health data and identifying the most important factors that contribute to health outcomes. [@oualy2021income] investigated the relationship between income inequality and socio-political instability in sub-Saharan Africa using PCA. The author used PCA to construct an index of socio-political instability based on political violence, government stability, and human rights violations. The study demonstrated that PCA can be an effective method for combining multiple indicators into a single index and identifying the underlying factors that contribute to socio-political instability. In the field of Finance, [@fang2018stable] proposed a stable systemic risk ranking method for China's banking sector based on PCA. The authors applied PCA to a set of variables related to the financial performance of Chinese banks and identified the most significant principal components that explain the variance in the data. The study showed how PCA can be used to measure systemic risk in the banking sector and to identify the most important factors that contribute to financial stability. Finally, PCA is often used in Climate and weather analysis. [jolliffe1990principal] used PCA to reduce dimensionality related to meteorology and climatology. The author outlines the importance of interpreting the PCA results in the context of the original data used. In another article related to climate, [@horel1984complex] used PCA for certain geophysical phenomena where cross-spectral analysis comes up short. The author used complex time series data to compute complex PCs from cross-covariance and cross-correlation matrices.

The examples above show the use of PCA in several fields. Our focus is the use of PCA in sports. Due to the complex nature of sports statistics, PCA has been used by many authors in the past to reduce the dimensionality of sports data sets. [@moura2014analysis] analyzed European football game-related team statistics using PCA and clustering analysis. The authors applied PCA to a set of variables related to offensive and defensive performance and identified the most significant principal components that explain the variance in the data. The study showed that PCA can be a useful tool for summarizing complex sports data and identifying the most important factors that contribute to overall team performance. An article by [@pino2021training] conducted a review of the most relevant variables for training design, performance analysis, and talent identification in European football, basketball, and rugby using PCA. The authors applied PCA to a set of variables related to physical fitness, technical skills, and tactical performance and identified the most significant principal components that explain the variance in the data. The study showed that PCA can be a useful tool for summarizing complex sports data and identifying the most important factors that contribute to performance. Like Pino-Ortega’s article, Jones 2016 used PCA to analyze individual player’s performance in the United States College Football League [@jones2016modeling]. The author used Principal Components Analysis to try to find a better statistical method to rank professional quarterback draft prospects than the commonly used Quarterback Rating. The target variable was log transformed. After PCA, the model included only two variables after starting from a very large dataset.

Our interest was also in sports data. We decided to use a dataset using the American National Football League for our analysis. This too has also been used many times in previous studies. Watnik et. al. 2001 used a PCA analysis completed on an NFL dataset from the year 2000 regular season [@watnik2001nfl]. PCA was used to determine each team’s strength. The PCA reduced the dataset to 2 Principal components. The authors found their best regression, with an R square value of 83% when using only the AFC teams. Using all teams and all variables resulted in a R square of only 21%.

While football has remained one of the most watched sports for some time, the popularity of fantasy sports has exploded more recently. To capitalize on fantasy sports, several betting companies have started hosting fantasy football competitions. With these competitions comes the use of analysis to find a competitive edge. With the large datasets involved, Sugar et. al. attempts a PCA to reduce dimensionality and extract the first 2 principal eigenvectors [@sugar2015predicting]. The author then projected the 7-dimensional RB and WR scaled data onto the reduced 2-dimensional PCA space. Visualizing the PCA the author sees that the distribution of WRs and RBs suggests that the x-axis roughly corresponds to a player’s receiving ability, while the y-axis corresponds to their rushing ability. When used with clustering analysis, the PCA values show a way to generate more effective subsets without the risk of overfitting the models. The authors best results show that they were able to reduce the training set for the model by a factor of 3 without significantly increasing variance.

In conclusion, PCA is a powerful statistical method for reducing the dimensionality of multivariate data and identifying the most significant factors that contribute to the variance in the data. This literature review demonstrated the wide range of applications of PCA in various fields, including remote sensing, public health, sports, and finance. PCA can be a useful tool for summarizing complex data, identifying the most important factors that contribute to the outcomes of interest, and providing insights for decision-making.

### Methods

Principal Component Analysis (PCA) is an unsupervised machine learning technique for reducing the dimensionality of multivariate data. A PCA analysis allows us to reduce the number of variables in a dataset, which allows us to easily identify relationships between the variables. A byproduct of this is easily visualizable data. 
	The first step in a PCA is to normalize the data. A PCA needs normalized data because if data was based off of actual numerical values we would have the larger variables unfairly influence the results of the analysis. We then need to calculate the covariance matrix for the dataset. The covariance matrix is used to determine how correlated the variables in a dataset are. We then would need to calculate the eigenvalues and eigenvectors of the covariance matrix. The eigenvectors are used to determine the direction of the principal components. While the eigenvalues are used to determine the strength of the principal components. After we determine the eigenvalues and vectors,  we have to find how many principal components to keep. This varies greatly depending on the goals of the analysis. In this analysis we determined that two was adequate. After finding the principal components we can interpret the results.

### Old Methods

PCA leverages an unsupervised linear transformation to perform feature extraction and dimensionality reduction. It is recommended to use PCA when dealing with strongly correlated variables. The PCA that we are running will be used to explore the dataset.  

In order to run a PCA analysis we first had to normalize the data. The data we were able to get offline was incomplete for some quarterbacks. We had to fill in the stats with zero if that variable had been blank. We then had to remove any variable with any nonlinear data. We then normalized the data using the scale function. After the data was normalized we could move onto the actual analysis. 

After the dataset was normalized we ran it through a correlation matrix. A correlation matrix is a table used to summarize data to compute a PCA (or other analyses). The correlation matrix turns the variables into a nxn matrix. In this case there is 50 variables, so it would be a 50x50 matrix  This table displays the correlation coefficients between the variables in a data set.

The correlation matrix is then run through the principal component to get the PCA values. The summary table shows the results of the PCA values. 

### Data

https://www.pro-football-reference.com/years/2022/passing.htm
https://www.pro-football-reference.com/years/2022/passing_advanced.htm

We are looking at advanced passing stats for the 2022 - 2023 NFL calendar year. The data was found on pro-football-reference.com. We looked at common NFL stats such as touchdowns, interceptions, and yards while also looking at less used stats such as on target % and how much time they had in the pocket. By having over fifty variables a PCA is the best way to summarize the large data set.  


```{r}
our_data = read.csv("NFL_Data.csv")

knitr::kable(
  our_data[1:3, 1:10], caption = 'Original 2022 NFL Data.'
)
```

Below there is a tabel describing the variables

| Variable | Description | Type |
|:---------|:-----------:|-----:|
| G        | Games Played | Numeric |
| GS       | Games Started | Numeric |
| Cmp      | Completed Passes | Numeric |
| Att      | Attempted Passes | Numeric |
| Cmp%     | Percentage of Attempts Completed | Numeric |
| Yds      | Yards Gained by Passing | Numeric |
| TD       | Touchdowns Scored by Passing | Numeric |
| TD%      | Percentage of Attempts Scoring TD | Numeric |
| Int      | Attempt Intercepted by Defense | Numeric |
| Int%     | Percentage of Attempts Intercepted | Numeric |
| 1D       | Attempt Resulted in a First Down | Numeric |
| Lng      | Longest Completed Pass | Numeric |
| Y/A      | Yard gained per Attempt | Numeric |
| AY/A     | Adjusted Yards per Attempt | Numeric |
| Y/C      | Yards Gained per Completion | Numeric |
| Y/G      | Yards Gained per Game | Numeric |
| Rate     | Passer Rating | Numeric |
| QBR      | Quarterback Rating | Numeric |
| Sk       | Sacks taken by Player | Numeric |
| Sk Yds   | Yards lost from Sacks | Numeric |
| Sk%      | Percentage of Plays resulting in sack | Numeric |
| NY/A     | Net Yards per Attempt | Numeric |
| ANY/A    | Adjusted Net Yards per Attempt | Numeric |
| 4QC      | Fourth Quarter Comebacks | Numeric |
| GWD      | Game-Winning Drives Led by Quarterback | Numeric |
| IAY      | Intended Air Yards (Completed and Incompleted) | Numeric |
| IAY/PA   | Intended Air Yards per Attempt | Numeric |
| CAY      | Completed Air Yards | Numeric |
| CAY/Cmp  | Completed Air Yards per Completion | Numeric |
| CAY/PA   | Completed Air Yards per Attempt | Numeric |
| YAC      | Yards Gained After Catch | Numeric |
| YAC/Cmp  | Yards After Catch per Completion | Numeric |
| Bats     | Passes Batted Down by Defense | Numeric |
| ThAwy    | Passes Thrown Away y Defense | Numeric |
| Spikes   | Spikes | Numeric |
| Drops    | Passes Dropped by Receivers | Numeric |
| Drop%    | Drop Percentage | Numeric |
| BadTh    | Incomplete Passes due to Bad Throws | Numeric |
| Bad%     | Bad Throw Percentage | Numeric |
| OnTgt    | On Target Throws | Numeric |
| OnTgt%   | On Target Throws Percentage | Numeric |
| PktTime  | Time In | Numeric |
| Bltz     | Times the Player Faced Blitzes | Numeric |
| Hrry     | Hurries | Numeric |
| Hits     | Times Hit by Defense | Numeric |
| Prss     | Plays with Quaterback Pressures | Numeric |
| Prss%    | Percentage of Plays Pressured | Numeric |
| Scrm     | Pass Plays resulting in the Quarterback Rushing | Numeric |
| Yds/Scr  | Rush Yards per Scramble | Numeric |

### Statistical Modeling

```{r}
#| label: fig-corr
#| fig-cap:
#|   - "Correlation Matrix"
our_data[is.na(our_data)] = 0
numerical_data = our_data[,2:51]
data_normalized = scale(numerical_data)
corr_matrix = cor(data_normalized)
ggcorrplot(corr_matrix, tl.cex = 5,tl.srt = 45)
  
```


@fig-corr looks at the correlation between variables

```{r}
#| label: fig-Scree
#| fig-cap: 
#|   - "Scree Plot"

data.pca = princomp(corr_matrix)
fviz_eig(data.pca, addlabels = TRUE) #this is a scree plot of the principal component
```

This first plot is a scree plot. This is used to visualize the importance of each principal component and can be used to determine the number of components to keep. This plots the eigenvalues each as a bar graph in a downward curve, from highest to lowest. The first two components can be considered the most significant since they contain almost 91% of the total information of the data.

```{r}
#| label: fig-biplot
#| fig-cap: 
#|   - "Biplot"

fviz_pca_var(data.pca, col.var = "black")
```

Figure 3 is a biplot of the attributes. The biplot can visualize the similarities and dissimilarities between the samples, and further shows the impact of each attribute on each principal component. The biplot observes three key pieces of information: all the variables that are grouped together are positively correlated to each other, the higher the distance between the variable and the origin, the better represented that variable is, and variables that are negatively correlated are displayed to the opposite sides of the biplot’s origin. Our graph has 2 large groups that are correlated with each other. The far right has a lot of overlap. There are also a lot of variables below the origin that are all negatively correlated together. The right have the most representation being further from the origin than others.

```{r}
#| label: fig-cos
#| fig-cap: 
#|   - "Cos2 graph"

fviz_cos2(data.pca, choice = "var", axes = 1:2)
```

The third graph is a Cos2 graph and corresponds to the square cosine. This visualization is to determine how much each variable is represented in a given component. A low value means that the variable is not perfectly represented by that component. A high value, on the other hand, means a good representation of the variable on that component. Our graph shows that we have 10 variables that have a good representation in both principal components. The graph looks like it has 4 different groups where the quality of representation drop from one group to the next. You can see at the end the last six variables have a lower quality.  

```{r}
#| label: fig-comb
#| fig-cap: 
#|   - "Cos2 graph"

fviz_pca_var(data.pca, col.var = "cos2",
            gradient.cols = c("goldenrod1", "indianred",  "cornflowerblue"),
            repel = TRUE)
```
This graph is a combination of the last 2 graphs, the biplot and the Cos2. This uses the biplot graph and then enhances it with color based on the Cos2 values. This has 3 colors, but you can see them start to blend in areas where the Cos2 values are similar between the groups we talked about before. The colors show that the best representation of variables are the blue/purple on the far right, and this is best represented by Dim1 or principal component1. The red values are the next best representation and  are best represented in the second principal component.    

### Conclusion

In conclusion,there really isn't a perfect way to choose the number of principal components are needed/wanted. One common method is to examine the scree plot. Thus, to identify where the point of the proportion of variance explained drops significantly, or what is referred to at the elbow in the plot. Another way is to decide on the proportion of variance that is explained as a cutoff, like 90%. In our data, you can see that the first two principal components achieve both of these benchmarks. This makes choosing the first 2 easy, as they account for 91% of the proportion of the variance explained and the drop off is fairly significant to the third principal component. What we have accomplished is starting with 50 variables and reducing the dimensionality all the way to two variables that account for 91% of the variance explained. Next steps would be to transform the original data by the principal components. That is FinalDataSet=Transpose(FeatureVector)*Transpose(StandardizedOriginalDataSet), where the Feature Vector is simply a matrix that has the eigenvectors of the components that were kept as columns. After the transformation, this will reduce the dimensionality and then an easier, more cost efficient, and less needed computer resources analysis can be performed.  

### References

